{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1 = pd.read_csv('cb1.csv')\n",
    "cb2 = pd.read_csv('cb2.csv')\n",
    "cb3 = pd.read_csv('cb3.csv')\n",
    "cb = cb1.append(cb2)\n",
    "cb = cb.append(cb3)\n",
    "cb.index = range(len(cb))\n",
    "cb['date_my'] = (pd.to_datetime(cb['date'],format='%m/%d/%Y', errors='coerce')).dt.to_period('m')\n",
    "cb['date_y'] = (pd.to_datetime(cb['date'],format='%m/%d/%Y', errors='coerce')).dt.to_period('y')\n",
    "\n",
    "nodes_org = cb[['target_name', 'target_ID', 'target_country', 'target_continent','industry']].drop_duplicates()\n",
    "nodes_org['bipartite'] = 1\n",
    "nodes_inv = cb[['investors_name', 'investor_id', 'investor_country', 'investor_continent', 'investor_type']].drop_duplicates()\n",
    "nodes_org = nodes_org.rename(columns={\"target_name\": \"node\", \"target_country\": \"country\", \n",
    "                                      \"target_continent\": \"continent\", \"industry\":\"industry\",\n",
    "                                     \"target_ID\":\"index\"})\n",
    "nodes_inv = nodes_inv.rename(columns={\"investors_name\": \"node\",\"investor_country\": \"country\", \n",
    "                                      \"investor_continent\": \"continent\", \"investor_type\":\"industry\",\"investor_id\":'index' })\n",
    "nodes_inv['bipartite'] = 0\n",
    "nodes = nodes_org.append(nodes_inv, ignore_index= True)\n",
    "nodes['id'] = nodes.index\n",
    "\n",
    "numbers = cb.groupby(['target_name','stage'], as_index = False).count()[['target_name','stage','investors_name']].rename(columns={\"investors_name\": \"investor_numbers\"})\n",
    "cb_new = pd.merge(cb, numbers,  how='left', left_on=['target_name','stage'], right_on = ['target_name','stage'])\n",
    "cb_new = pd.merge(cb_new, nodes[nodes['bipartite']== 1],  how='left', left_on=['target_name','target_ID'], right_on = ['node','index'])\n",
    "cb_new = pd.merge(cb_new, nodes[nodes['bipartite']== 0],  how='left', left_on=['investors_name','investor_id'], right_on = ['node','index'])\n",
    "nodes = nodes.drop(columns = \"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_new['target_founded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv('nodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_fda = pd.read_csv('traject_3y.csv')\n",
    "#covariates = pd.read_csv(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\Covariate_bidbid_final_final2.csv')\n",
    "covariates_org = pd.read_csv(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\Covariate_orgorg5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = list(nodes_fda.columns)\n",
    "orgs = [int(x) for x in orgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities = list(covariates.columns)\n",
    "for i in range(0,len(centralities)):\n",
    "    centralities[i] = centralities[i][:-4]\n",
    "centralities = list(set(centralities))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_org = list(covariates_org.columns)\n",
    "for i in range(0,len(centralities_org)):\n",
    "    centralities_org[i] = centralities_org[i][:-4]\n",
    "centralities_org = list(set(centralities_org))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_org['id'] = covariates_org['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTRALITITES OF ALL\n",
    "\n",
    "sub0 = cb_new[['id_x','date_y','id_y', 'size_real']]\n",
    "#for cen in centralities: \n",
    "#    locals()[cen+'_max'] = []\n",
    "#    locals()[cen+'_min'] = []\n",
    "#    locals()[cen+'_median'] = []\n",
    "#operators = ['_max', '_min', '_median']  \n",
    "#num_inv = []\n",
    "\n",
    "s = 0\n",
    "\n",
    "for cen_org in centralities_org:\n",
    "    locals()[cen_org +'_org'] = []\n",
    "\n",
    "for  i in tqdm(orgs):\n",
    "    \n",
    "    sub1 = sub0[sub0['id_x']==i]\n",
    "    sub1 = sub1.dropna()\n",
    "    sub2 = sub1[sub1['date_y']==sub1['date_y'].values[0]]\n",
    "    anno = sub2['date_y'].values[0].year\n",
    "    \n",
    "\n",
    "    for cen_org in centralities_org:\n",
    "        try:\n",
    "            locals()[cen_org + '_org'].append((covariates_org[covariates_org['id']== i][cen_org + str(anno)].values[0]))\n",
    "        except:\n",
    "            locals()[cen_org + '_org'].append(np.nan)\n",
    "        \n",
    "    s += 1\n",
    "    if s % 500 == 0:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate5= pd.DataFrame(nodes[(nodes['id'].isin(orgs))&(nodes['bipartite']==1)]['id'])\n",
    "\n",
    "\n",
    "for cen_org in centralities_org:\n",
    "    covariate5[cen_org + '_org'] = locals()[cen_org + '_org']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = pd.read_csv('covariates.csv')\n",
    "cov_bid = covariates.iloc[:,1:44]\n",
    "covariates10 =pd.merge(covariate10, cov_bid, left_on='id', right_on='id')\n",
    "covariates10.to_csv('covariates10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nx.readwrite.read_gpickle(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\orgorg6\\orgorg6_single2020.gpickle')\n",
    "b = nx.readwrite.read_gpickle(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\orgorg5\\orgorg5_single2020.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nx.readwrite.read_gpickle(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\orgorg4\\orgorg4_single2020.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = list(set(covariate['industry']))\n",
    "settori = {'industry' : industries, \n",
    "           'sector' : ['Healthcare', 'Healthcare', 'Healthcare', 'Healthcare','Healthcare','Healthcare','Healthcare','Healthcare','Healthcare', 'Consumer Products & Services', 'Healthcare', 'Software (non-internet/mobile)',\n",
    "                                     'Internet', 'Healthcare','Healthcare','Healthcare','Healthcare','Healthcare', 'Mobile & Telecommunications', 'Healthcare','Healthcare','Healthcare']\n",
    "}\n",
    "\n",
    "settori = pd.DataFrame(settori)\n",
    "\n",
    "foundation = cb_new[['id_x', 'target_founded']].drop_duplicates()\n",
    "\n",
    "covariate2 =pd.merge(covariate, foundation, left_on='id', right_on='id_x')\n",
    "covariate2 =pd.merge(covariate2, settori, left_on='industry', right_on='industry')\n",
    "\n",
    "covariate2 = covariate2.drop('id_x', axis =1)\n",
    "\n",
    "foundation = covariate2.pop('target_founded')\n",
    "sector = covariate2.pop('sector')\n",
    "\n",
    "covariate2.insert(4, 'foundation', foundation)\n",
    "covariate2.insert(5, 'sector', sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub0 = cb_new[['id_x','date_y','round_simp','size_real','target_status']].drop_duplicates()\n",
    "\n",
    "ids = covariate2['id']\n",
    "stage = []\n",
    "first_money = []\n",
    "date = []\n",
    "situa = []\n",
    "\n",
    "for i in tqdm(ids):\n",
    "    sub1 = sub0[sub0['id_x'] == i]\n",
    "    sub1 = sub1.dropna()\n",
    "    stage.append(sub1['round_simp'].values[0])\n",
    "    first_money.append(sub1['size_real'].values[0])\n",
    "    situa.append(sub1['target_status'].values[0])\n",
    "    date.append(sub1['date_y'].values[0])\n",
    "    \n",
    "    \n",
    "stag = {'id' : list(ids),\n",
    "       'stage' : stage,\n",
    "       'first_money': first_money,\n",
    "       'date' : date,\n",
    "       'current_sit' : situa}\n",
    "stag = pd.DataFrame(stag)\n",
    "covariate2 =pd.merge(covariate2, stag, left_on='id', right_on='id')\n",
    "stag = covariate2.pop('stage')\n",
    "covariate2.insert(6, 'stage', stag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate2.to_csv('covariates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_network(lista_storage,lista_nomi,funzione,directed=False):\n",
    "    for year in tqdm(range(1990,2019)):\n",
    "        string='orgorg4_single'+str(year)+'.gpickle'\n",
    "        network=nx.readwrite.read_gpickle(string)\n",
    "        storage=str(funzione)+'_orgorg3_single'+str(year)\n",
    "        lista_storage.append(funzione(network))\n",
    "        lista_nomi.append(storage)\n",
    "#         network=nx.readwrite.read_gpickle('bidbid_stage_self_cb'+str(year)+'.gpickle')\n",
    "#         lista_storage.append(funzione(network))\n",
    "#         lista_nomi.append(str(funzione)+'_bidder_'+str(year))\n",
    "    return lista_storage,lista_nomi\n",
    "def apply_function_network_community(lista_storage,lista_nomi,funzione,directed=False):\n",
    "    for year in tqdm(range(1999,2020)):\n",
    "        string='orgorg3_single_/orgorg3_single'+str(year)+'.gpickle'\n",
    "        network=nx.readwrite.read_gpickle(string)\n",
    "        storage='Louvain_orgorg3_single'+str(year)\n",
    "        lista_storage.append(funzione(network))\n",
    "        lista_nomi.append(storage)\n",
    "    #network=nx.readwrite.read_gpickle('bidbid_stage_single_cb.gpickle')\n",
    "    #lista_storage.append(funzione(network))\n",
    "    #lista_nomi.append(str(funzione)+'_single')\n",
    "    return lista_storage,lista_nomi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_funzioni_def=[nx.betweenness_centrality,nx.harmonic_centrality,nx.clustering, nx.core_number,nx.eigenvector_centrality, nx.degree,nx.closeness_centrality,nx.pagerank,nx.load_centrality,nx.average_neighbor_degree]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_storage,lista_nomi=apply_function_network(lista_storage,lista_nomi,nx.betweenness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array di funzioni da applicare - ciclo for su di esse\n",
    "#lista_funzioni=[nx.voterank, nx.eigenvector_centrality, nx.degree_centrality, nx.pagerank, nx.closeness_centrality, nx.incremental_closeness_centrality, nx.load_centrality, nx.subgraph_centrality, nx.dispersion, nx.percolation_centrality,nx.average_neighbor_degree,nx.constraint, nx.betweenness_centrality]\n",
    "# lista_funzioni2=[nx.dispersion,nx.constraint,nx.betweenness_centrality, nx.trophic_levels, nx.harmonic_centrality, nx.enumerate_all_cliques, nx.clustering, nx.core_number, nx.rich_club_coefficient, nx.closeness_vitality]\n",
    "# lista_funzioni_rifare=[nx.harmonic_centrality, nx.enumerate_all_cliques, nx.clustering,nx.core_number,nx.closeness_vitality]\n",
    "# Funzione prende df, se direzionato o meno\n",
    "# lista_funzioni_prova2=[nx.eigenvector_centrality, nx.degree]\n",
    "lista_storage=[]\n",
    "lista_nomi=[]\n",
    "for funzione in lista_funzioni_def:\n",
    "    try:\n",
    "        print('Trying'+str(funzione))\n",
    "        lista_storage2, lista_nomi2=apply_function_network(lista_storage2,lista_nomi2,nx.betweenness_centrality)\n",
    "    except:\n",
    "        print(str(funzione)+' did not work')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=nx.readwrite.read_gpickle('orgorg4_single2006.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_network(funzione):\n",
    "    for year in tqdm(range(1990,2021)):\n",
    "        string2='orgorg4_single'+str(year)+'.gpickle'\n",
    "        network2=nx.readwrite.read_gpickle(string2)\n",
    "        storage2=str(funzione)+'_orgorg4_single'+str(year)+'.csv'\n",
    "        covariate2 = funzione(network2)\n",
    "        covariate3 = pd.Series(covariate2)\n",
    "        print('Arrivo qui', year)\n",
    "        covariate3.to_csv(storage2)\n",
    "        print('Done', funzione, 'for year', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for function in lista_funzioni_def:\n",
    "    apply_function_network(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # results = [executor.submit(apply_function_network,funzione) for funzione in lista_funzioni]\n",
    "    executor.map(apply_function_network, lista_funzioni_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_storage2=[]\n",
    "lista_nomi2=[]\n",
    "apply_function_network(lista_storage2,lista_nomi2,nx.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nomi2[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
