{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cb1 = pd.read_csv('cb1.csv')\n",
    "cb2 = pd.read_csv('cb2.csv')\n",
    "cb3 = pd.read_csv('cb3.csv')\n",
    "cb = cb1.append(cb2)\n",
    "cb = cb.append(cb3)\n",
    "cb.index = range(len(cb))\n",
    "cb['date_my'] = (pd.to_datetime(cb['date'],format='%m/%d/%Y', errors='coerce')).dt.to_period('m')\n",
    "cb['date_y'] = (pd.to_datetime(cb['date'],format='%m/%d/%Y', errors='coerce')).dt.to_period('y')\n",
    "\n",
    "nodes_org = cb[['target_name', 'target_ID', 'target_country', 'target_continent','industry']].drop_duplicates()\n",
    "nodes_org['bipartite'] = 1\n",
    "nodes_inv = cb[['investors_name', 'investor_id', 'investor_country', 'investor_continent', 'investor_type']].drop_duplicates()\n",
    "nodes_org = nodes_org.rename(columns={\"target_name\": \"node\", \"target_country\": \"country\", \n",
    "                                      \"target_continent\": \"continent\", \"industry\":\"industry\",\n",
    "                                     \"target_ID\":\"index\"})\n",
    "nodes_inv = nodes_inv.rename(columns={\"investors_name\": \"node\",\"investor_country\": \"country\", \n",
    "                                      \"investor_continent\": \"continent\", \"investor_type\":\"industry\",\"investor_id\":'index' })\n",
    "nodes_inv['bipartite'] = 0\n",
    "nodes = nodes_org.append(nodes_inv, ignore_index= True)\n",
    "nodes['id'] = nodes.index\n",
    "\n",
    "numbers = cb.groupby(['target_name','stage'], as_index = False).count()[['target_name','stage','investors_name']].rename(columns={\"investors_name\": \"investor_numbers\"})\n",
    "cb_new = pd.merge(cb, numbers,  how='left', left_on=['target_name','stage'], right_on = ['target_name','stage'])\n",
    "cb_new = pd.merge(cb_new, nodes[nodes['bipartite']== 1],  how='left', left_on=['target_name','target_ID'], right_on = ['node','index'])\n",
    "cb_new = pd.merge(cb_new, nodes[nodes['bipartite']== 0],  how='left', left_on=['investors_name','investor_id'], right_on = ['node','index'])\n",
    "nodes = nodes.drop(columns = \"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv('nodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_fda = pd.read_csv('traject_3y.csv')\n",
    "covariates = pd.read_csv(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\Covariate_bidbid_final_final2.csv')\n",
    "covariates_org = pd.read_csv(r'C:\\Users\\Marco\\Documents\\GitHub\\crunch_net\\ANS\\ANS_all\\Covariates_orgorg_final2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = list(nodes_fda.columns)\n",
    "orgs = [int(x) for x in orgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities = list(covariates.columns)\n",
    "for i in range(0,len(centralities)):\n",
    "    centralities[i] = centralities[i][:-4]\n",
    "centralities = list(set(centralities))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_org = list(covariates_org.columns)\n",
    "for i in range(0,len(centralities_org)):\n",
    "    centralities_org[i] = centralities_org[i][:-4]\n",
    "centralities_org = list(set(centralities_org))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea7c34147d3412f97025f8c0f75c225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_15464\\3412582721.py:42: RuntimeWarning: All-NaN axis encountered\n",
      "  locals()[cen+'_max'].append(np.nanmax(locals()[cen]))\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_15464\\3412582721.py:43: RuntimeWarning: All-NaN axis encountered\n",
      "  locals()[cen+'_min'].append(np.nanmin(locals()[cen]))\n",
      "c:\\python38\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1119: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "#CENTRALITITES OF ALL\n",
    "\n",
    "sub0 = cb_new[['id_x','date_y','id_y', 'size_real']]\n",
    "for cen in centralities: \n",
    "    locals()[cen+'_max'] = []\n",
    "    locals()[cen+'_min'] = []\n",
    "    locals()[cen+'_median'] = []\n",
    "operators = ['_max', '_min', '_median']  \n",
    "num_inv = []\n",
    "\n",
    "s = 0\n",
    "\n",
    "for cen_org in centralities_org:\n",
    "    locals()[cen_org +'_org'] = []\n",
    "\n",
    "for  i in tqdm(orgs):\n",
    "    \n",
    "    sub1 = sub0[sub0['id_x']==i]\n",
    "    sub1 = sub1.dropna()\n",
    "    sub2 = sub1[sub1['date_y']==sub1['date_y'].values[0]]\n",
    "    anno = sub2['date_y'].values[0].year\n",
    "    \n",
    "    for z in ['inv', 'org']:\n",
    "        if z == 'org':\n",
    "            for cen_org in centralities_org:\n",
    "                try:\n",
    "                    locals()[cen_org + '_org'].append((covariates_org[covariates_org['id']== i][cen_org + str(anno)].values[0]))\n",
    "                except:\n",
    "                    locals()[cen_org + '_org'].append(np.nan)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            investors = set(sub2['id_y'])\n",
    "            for cen in centralities:\n",
    "                locals()[cen] = []\n",
    "                for l in investors:\n",
    "                    try:\n",
    "                        locals()[cen].append((covariates[covariates['id']== l][cen + str(anno)].values[0]))\n",
    "                    except:\n",
    "                        locals()[cen].append(np.nan)\n",
    "                locals()[cen+'_max'].append(np.nanmax(locals()[cen]))\n",
    "                locals()[cen+'_min'].append(np.nanmin(locals()[cen]))\n",
    "                locals()[cen+'_median'].append(np.nanmedian(locals()[cen]))\n",
    "\n",
    "\n",
    "\n",
    "            num_inv.append(len(investors))\n",
    "        \n",
    "        s += 1\n",
    "        if s % 500 == 0:\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_15464\\1417526639.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  covariate[cen + op] = locals()[cen+op]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_15464\\1417526639.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  covariate['num_inv'] = num_inv\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_15464\\1417526639.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  covariate[cen_org + '_org'] = locals()[cen_org + '_org']\n"
     ]
    }
   ],
   "source": [
    "covariate = nodes[(nodes['id'].isin(orgs))&(nodes['bipartite']==1)]\n",
    "\n",
    "for cen in centralities:\n",
    "    for op in operators:\n",
    "        covariate[cen + op] = locals()[cen+op]\n",
    "covariate['num_inv'] = num_inv\n",
    "\n",
    "for cen_org in centralities_org:\n",
    "    covariate[cen_org + '_org'] = locals()[cen_org + '_org']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate.to_csv('covariates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = list(set(covariate['industry']))\n",
    "settori = {'industry' : industries, \n",
    "           'sector' : ['Healthcare', 'Healthcare', 'Healthcare', 'Healthcare','Healthcare','Healthcare','Healthcare','Healthcare','Healthcare', 'Consumer Products & Services', 'Healthcare', 'Software (non-internet/mobile)',\n",
    "                                     'Internet', 'Healthcare','Healthcare','Healthcare','Healthcare','Healthcare', 'Mobile & Telecommunications', 'Healthcare','Healthcare','Healthcare']\n",
    "}\n",
    "\n",
    "settori = pd.DataFrame(settori)\n",
    "\n",
    "foundation = cb_new[['id_x', 'target_founded']].drop_duplicates()\n",
    "\n",
    "covariate2 =pd.merge(covariate, foundation, left_on='id', right_on='id_x')\n",
    "covariate2 =pd.merge(covariate2, settori, left_on='industry', right_on='industry')\n",
    "\n",
    "covariate2 = covariate2.drop('id_x', axis =1)\n",
    "\n",
    "foundation = covariate2.pop('target_founded')\n",
    "sector = covariate2.pop('sector')\n",
    "\n",
    "covariate2.insert(4, 'foundation', foundation)\n",
    "covariate2.insert(5, 'sector', sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeb850c75b840cf865413f4d46b14a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub0 = cb_new[['id_x','date_y','round_simp','size_real','target_status']].drop_duplicates()\n",
    "\n",
    "ids = covariate2['id']\n",
    "stage = []\n",
    "first_money = []\n",
    "date = []\n",
    "situa = []\n",
    "\n",
    "for i in tqdm(ids):\n",
    "    sub1 = sub0[sub0['id_x'] == i]\n",
    "    sub1 = sub1.dropna()\n",
    "    stage.append(sub1['round_simp'].values[0])\n",
    "    first_money.append(sub1['size_real'].values[0])\n",
    "    situa.append(sub1['target_status'].values[0])\n",
    "    date.append(sub1['date_y'].values[0])\n",
    "    \n",
    "    \n",
    "stag = {'id' : list(ids),\n",
    "       'stage' : stage,\n",
    "       'first_money': first_money,\n",
    "       'date' : date,\n",
    "       'current_sit' : situa}\n",
    "stag = pd.DataFrame(stag)\n",
    "covariate2 =pd.merge(covariate2, stag, left_on='id', right_on='id')\n",
    "stag = covariate2.pop('stage')\n",
    "covariate2.insert(6, 'stage', stag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate2.to_csv('covariates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_function_network(lista_storage,lista_nomi,funzione,directed=False):\n",
    "    for year in tqdm(range(1990,2019)):\n",
    "        string='orgorg4_single'+str(year)+'.gpickle'\n",
    "        network=nx.readwrite.read_gpickle(string)\n",
    "        storage=str(funzione)+'_orgorg3_single'+str(year)\n",
    "        lista_storage.append(funzione(network))\n",
    "        lista_nomi.append(storage)\n",
    "#         network=nx.readwrite.read_gpickle('bidbid_stage_self_cb'+str(year)+'.gpickle')\n",
    "#         lista_storage.append(funzione(network))\n",
    "#         lista_nomi.append(str(funzione)+'_bidder_'+str(year))\n",
    "    return lista_storage,lista_nomi\n",
    "def apply_function_network_community(lista_storage,lista_nomi,funzione,directed=False):\n",
    "    for year in tqdm(range(1999,2020)):\n",
    "        string='orgorg3_single_/orgorg3_single'+str(year)+'.gpickle'\n",
    "        network=nx.readwrite.read_gpickle(string)\n",
    "        storage='Louvain_orgorg3_single'+str(year)\n",
    "        lista_storage.append(funzione(network))\n",
    "        lista_nomi.append(storage)\n",
    "    #network=nx.readwrite.read_gpickle('bidbid_stage_single_cb.gpickle')\n",
    "    #lista_storage.append(funzione(network))\n",
    "    #lista_nomi.append(str(funzione)+'_single')\n",
    "    return lista_storage,lista_nomi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lista_funzioni_def=[nx.betweenness_centrality,nx.harmonic_centrality,nx.clustering, nx.core_number,nx.eigenvector_centrality, nx.degree,nx.closeness_centrality,nx.pagerank,nx.load_centrality,nx.average_neighbor_degree]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab63e09e7ef942619d535b4301cfab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lista_storage,lista_nomi=apply_function_network(lista_storage,lista_nomi,nx.betweenness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying<function betweenness_centrality at 0x7fe0d0398dc0>\n",
      "<function betweenness_centrality at 0x7fe0d0398dc0> did not work\n",
      "Trying<function harmonic_centrality at 0x7fe0d03bc4c0>\n",
      "<function harmonic_centrality at 0x7fe0d03bc4c0> did not work\n",
      "Trying<function clustering at 0x7fe0989614c0>\n",
      "<function clustering at 0x7fe0989614c0> did not work\n",
      "Trying<function core_number at 0x7fe098973af0>\n",
      "<function core_number at 0x7fe098973af0> did not work\n",
      "Trying<function eigenvector_centrality at 0x7fe0d03bc3a0>\n",
      "<function eigenvector_centrality at 0x7fe0d03bc3a0> did not work\n",
      "Trying<function degree at 0x7fe10090ac10>\n",
      "<function degree at 0x7fe10090ac10> did not work\n",
      "Trying<function closeness_centrality at 0x7fe0d03a3a60>\n",
      "<function closeness_centrality at 0x7fe0d03a3a60> did not work\n",
      "Trying<function pagerank at 0x7fe1009fc310>\n",
      "<function pagerank at 0x7fe1009fc310> did not work\n",
      "Trying<function newman_betweenness_centrality at 0x7fe0d03c71f0>\n",
      "<function newman_betweenness_centrality at 0x7fe0d03c71f0> did not work\n",
      "Trying<function average_neighbor_degree at 0x7fe0d0387280>\n",
      "<function average_neighbor_degree at 0x7fe0d0387280> did not work\n"
     ]
    }
   ],
   "source": [
    "# Array di funzioni da applicare - ciclo for su di esse\n",
    "#lista_funzioni=[nx.voterank, nx.eigenvector_centrality, nx.degree_centrality, nx.pagerank, nx.closeness_centrality, nx.incremental_closeness_centrality, nx.load_centrality, nx.subgraph_centrality, nx.dispersion, nx.percolation_centrality,nx.average_neighbor_degree,nx.constraint, nx.betweenness_centrality]\n",
    "# lista_funzioni2=[nx.dispersion,nx.constraint,nx.betweenness_centrality, nx.trophic_levels, nx.harmonic_centrality, nx.enumerate_all_cliques, nx.clustering, nx.core_number, nx.rich_club_coefficient, nx.closeness_vitality]\n",
    "# lista_funzioni_rifare=[nx.harmonic_centrality, nx.enumerate_all_cliques, nx.clustering,nx.core_number,nx.closeness_vitality]\n",
    "# Funzione prende df, se direzionato o meno\n",
    "# lista_funzioni_prova2=[nx.eigenvector_centrality, nx.degree]\n",
    "lista_storage=[]\n",
    "lista_nomi=[]\n",
    "for funzione in lista_funzioni_def:\n",
    "    try:\n",
    "        print('Trying'+str(funzione))\n",
    "        lista_storage2, lista_nomi2=apply_function_network(lista_storage2,lista_nomi2,nx.betweenness_centrality)\n",
    "    except:\n",
    "        print(str(funzione)+' did not work')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "network=nx.readwrite.read_gpickle('orgorg4_single2006.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_function_network(funzione):\n",
    "    for year in tqdm(range(1990,2021)):\n",
    "        string2='orgorg4_single'+str(year)+'.gpickle'\n",
    "        network2=nx.readwrite.read_gpickle(string2)\n",
    "        storage2=str(funzione)+'_orgorg4_single'+str(year)+'.csv'\n",
    "        covariate2 = funzione(network2)\n",
    "        covariate3 = pd.Series(covariate2)\n",
    "        print('Arrivo qui', year)\n",
    "        covariate3.to_csv(storage2)\n",
    "        print('Done', funzione, 'for year', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5273848ab064f6fab54c91be646cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrivo qui 1990\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1990\n",
      "Arrivo qui 1991\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1991\n",
      "Arrivo qui 1992\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1992\n",
      "Arrivo qui 1993\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1993\n",
      "Arrivo qui 1994\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1994\n",
      "Arrivo qui 1995\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1995\n",
      "Arrivo qui 1996\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1996\n",
      "Arrivo qui 1997\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1997\n",
      "Arrivo qui 1998\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1998\n",
      "Arrivo qui 1999\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 1999\n",
      "Arrivo qui 2000\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2000\n",
      "Arrivo qui 2001\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2001\n",
      "Arrivo qui 2002\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2002\n",
      "Arrivo qui 2003\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2003\n",
      "Arrivo qui 2004\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2004\n",
      "Arrivo qui 2005\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2005\n",
      "Arrivo qui 2006\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2006\n",
      "Arrivo qui 2007\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2007\n",
      "Arrivo qui 2008\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2008\n",
      "Arrivo qui 2009\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2009\n",
      "Arrivo qui 2010\n",
      "Done <function betweenness_centrality at 0x7fa37894dee0> for year 2010\n"
     ]
    }
   ],
   "source": [
    "for function in lista_funzioni_def:\n",
    "    apply_function_network(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c5eb66ec0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# results = [executor.submit(apply_function_network,funzione) for funzione in lista_funzioni]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_function_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlista_funzioni_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/process.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunksize must be >= 1.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         results = super().map(partial(_process_chunk, fn),\n\u001b[0m\u001b[1;32m    727\u001b[0m                               \u001b[0m_get_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                               timeout=timeout)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Yield must be hidden in closure so that the futures are submitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Yield must be hidden in closure so that the futures are submitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/process.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBrokenProcessPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_thread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot schedule new futures after shutdown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # results = [executor.submit(apply_function_network,funzione) for funzione in lista_funzioni]\n",
    "    executor.map(apply_function_network, lista_funzioni_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apply_function_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-367559f8b006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlista_storage2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlista_nomi2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapply_function_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_storage2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlista_nomi2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'apply_function_network' is not defined"
     ]
    }
   ],
   "source": [
    "lista_storage2=[]\n",
    "lista_nomi2=[]\n",
    "apply_function_network(lista_storage2,lista_nomi2,nx.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function degree at 0x7fb1412bf9d0>_orgorg3_single2000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_nomi2[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
